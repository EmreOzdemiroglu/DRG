#!/usr/bin/env python3
"""
GraphRAG Pipeline Example - Tam Pipeline Testi

GraphRAG yapÄ±sÄ±na uygun olarak:
1. Chunking
2. KG Extraction
3. Entity Embedding'leri Ekleme
4. Community Report OluÅŸturma
5. GraphRAG Retrieval (KG traversal + Community reports)

KullanÄ±m:
    python examples/graphrag_pipeline_example.py [example_name]

Ã–rnek:
    python examples/graphrag_pipeline_example.py 1
    python examples/graphrag_pipeline_example.py 1example
    python examples/graphrag_pipeline_example.py example1  # Otomatik 1example'a Ã§evrilir
"""

import os
import sys
import json
from pathlib import Path

# Note: SSL verification is enabled by default for security.
# If you encounter SSL certificate errors, you should:
# 1. Update your Python installation and certificates
# 2. Set REQUESTS_CA_BUNDLE or SSL_CERT_FILE environment variables to point to valid certificates
# 3. For development only (NOT recommended for production): Use environment variable
#    REQUESTS_CA_BUNDLE="" (empty) but understand the security risks
#
# DO NOT disable SSL verification in code as it creates security vulnerabilities
# (man-in-the-middle attacks, API key exposure, etc.)

# Proje root'u path'e ekle
sys.path.insert(0, str(Path(__file__).parent.parent))

from typing import Union

from drg.chunking import create_chunker
from drg.embedding import create_embedding_provider
from drg.extract import extract_typed, KGExtractor
from drg.schema import DRGSchema, EnhancedDRGSchema, Entity, Relation, EntityType, RelationGroup
from drg.graph import KG
from drg.graph.kg_core import EnhancedKG, KGNode, KGEdge
from drg.graph.community_report import CommunityReportGenerator
from drg.clustering import create_clustering_algorithm
from drg.retrieval import create_graphrag_retriever


def load_schema(schema_path: str) -> Union[DRGSchema, EnhancedDRGSchema]:
    """Schema'yÄ± JSON dosyasÄ±ndan yÃ¼kle (Enhanced veya legacy format destekler)."""
    with open(schema_path, "r", encoding="utf-8") as f:
        schema_data = json.load(f)
    
    # Enhanced schema formatÄ±nÄ± kontrol et
    if "entity_types" in schema_data:
        from drg.schema import EntityType, RelationGroup
        entity_types = [
            EntityType(
                name=et["name"],
                description=et.get("description", ""),
                examples=et.get("examples", []),
                properties=et.get("properties", {})
            )
            for et in schema_data["entity_types"]
        ]
        
        relation_groups = []
        for rg_data in schema_data.get("relation_groups", []):
            relations = [
                Relation(
                    name=r["name"],
                    src=r["source"],
                    dst=r["target"],
                    description=r.get("description", ""),
                    detail=r.get("detail", "")
                )
                for r in rg_data.get("relations", [])
            ]
            relation_groups.append(RelationGroup(
                name=rg_data["name"],
                description=rg_data.get("description", ""),
                relations=relations,
                examples=rg_data.get("examples", [])
            ))
        
        return EnhancedDRGSchema(
            entity_types=entity_types,
            relation_groups=relation_groups,
            auto_discovery=schema_data.get("auto_discovery", False)
        )
    else:
        # Legacy format
        entities = [Entity(e["name"]) for e in schema_data.get("entities", [])]
        relations = [
            Relation(
                r["name"], 
                r.get("source", r.get("src", "")), 
                r.get("target", r.get("dst", "")),
                description=r.get("description", ""),
                detail=r.get("detail", "")
            )
            for r in schema_data.get("relations", [])
        ]
        
        return DRGSchema(entities=entities, relations=relations)


def save_schema(schema: Union[DRGSchema, EnhancedDRGSchema], schema_path: str) -> None:
    """Schema'yÄ± JSON dosyasÄ±na kaydet."""
    from drg.schema import EnhancedDRGSchema as EDRGS
    from pathlib import Path
    
    schema_path_obj = Path(schema_path)
    schema_path_obj.parent.mkdir(parents=True, exist_ok=True)
    
    if isinstance(schema, EDRGS):
        # Enhanced schema formatÄ±
        schema_dict = {
            "entity_types": [
                {
                    "name": et.name,
                    "description": et.description,
                    "examples": et.examples,
                    "properties": et.properties
                }
                for et in schema.entity_types
            ],
            "relation_groups": [
                {
                    "name": rg.name,
                    "description": rg.description,
                    "relations": [
                        {
                            "name": r.name,
                            "source": r.src,
                            "target": r.dst,
                            "description": r.description,
                            "detail": r.detail
                        }
                        for r in rg.relations
                    ],
                    "examples": rg.examples
                }
                for rg in schema.relation_groups
            ],
            "auto_discovery": schema.auto_discovery
        }
    else:
        # Legacy format
        schema_dict = {
            "entities": [{"name": e.name} for e in schema.entities],
            "relations": [
                {
                    "name": r.name,
                    "source": r.src,
                    "target": r.dst,
                    "description": r.description,
                    "detail": r.detail
                }
                for r in schema.relations
            ]
        }
    
    with open(schema_path, "w", encoding="utf-8") as f:
        json.dump(schema_dict, f, indent=2, ensure_ascii=False)


def main():
    """GraphRAG pipeline'Ä±nÄ± test et."""
    
    # Example name belirle - sayÄ± baÅŸta formatÄ± kullan (1example, 2example, etc.)
    raw_name = sys.argv[1] if len(sys.argv) > 1 else "1"
    
    # EÄŸer sadece sayÄ± verilmiÅŸse "Nexample" formatÄ±na Ã§evir
    if raw_name.isdigit():
        example_name = f"{raw_name}example"
    elif raw_name.startswith("example"):
        # Eski format: "example1" -> "1example"
        num = raw_name.replace("example", "")
        example_name = f"{num}example" if num.isdigit() else raw_name
    else:
        example_name = raw_name
    
    print("=" * 70)
    print(f"GraphRAG Pipeline Test - {example_name.upper()}")
    print("=" * 70)
    
    # KlasÃ¶r yapÄ±sÄ±nÄ± oluÅŸtur
    inputs_dir = Path("inputs")
    outputs_dir = Path("outputs")
    inputs_dir.mkdir(exist_ok=True)
    outputs_dir.mkdir(exist_ok=True)
    
    # Schema ve text dosya yollarÄ± - sayÄ± baÅŸta format
    schema_path = inputs_dir / f"{example_name}_schema.json"
    text_path = inputs_dir / f"{example_name}_text.txt"
    
    # API Key ayarla (sadece environment variable'dan oku)
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    
    # Model seÃ§imi: OpenRouter (primary)
    if not os.getenv("DRG_MODEL"):
        if openrouter_key:
            os.environ["DRG_MODEL"] = "openrouter/anthropic/claude-3-haiku"
            print("âœ… OPENROUTER_API_KEY bulundu, OpenRouter model kullanÄ±lacak")
        else:
            os.environ["DRG_MODEL"] = "openrouter/anthropic/claude-3-haiku"
            print("âš ï¸  OPENROUTER_API_KEY bulunamadÄ±. OpenRouter varsayÄ±lan olarak kullanÄ±lacak.")
            print("   API key ayarlamak iÃ§in: export OPENROUTER_API_KEY='your-key'")
    
    # Metni dosyadan yÃ¼kle
    if not text_path.exists():
        print(f"âš ï¸  Metin dosyasÄ± bulunamadÄ±: {text_path}")
        print(f"   LÃ¼tfen {text_path} dosyasÄ±nÄ± oluÅŸturun")
        return
    
    print(f"ğŸ“„ Metin dosyasÄ± yÃ¼kleniyor: {text_path}")
    with open(text_path, "r", encoding="utf-8") as f:
        text = f.read()
    
    print(f"\nğŸ“„ Metin uzunluÄŸu: {len(text)} karakter")
    print(f"   Kelime sayÄ±sÄ±: {len(text.split())}")
    
    # Step 1: Chunking
    print("\n" + "=" * 70)
    print("1. CHUNKING")
    print("=" * 70)
    
    # Declarative chunking: preset kullanarak
    chunker = create_chunker(preset="graphrag")
    
    chunks = chunker.chunk(
        text=text,
        origin_dataset="apple_corpus",
        origin_file="apple_history.txt",
    )
    
    print(f"âœ… {len(chunks)} chunk oluÅŸturuldu")
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"   Chunk {i}: {chunk.token_count} tokens, ID: {chunk.chunk_id}")
    
    # Step 2: KG Extraction
    print("\n" + "=" * 70)
    print("2. KNOWLEDGE GRAPH EXTRACTION")
    print("=" * 70)
    
    # Schema generation stratejisi:
    # 1. EÄŸer DRG_FORCE_SCHEMA_GEN=1 ise, her zaman metinden schema oluÅŸtur (schema dosyasÄ±nÄ± yok say)
    # 2. EÄŸer schema dosyasÄ± yoksa, otomatik oluÅŸtur
    # 3. EÄŸer schema dosyasÄ± varsa ve DRG_FORCE_SCHEMA_GEN yoksa, dosyadan yÃ¼kle
    
    force_schema_gen = os.getenv("DRG_FORCE_SCHEMA_GEN", "0") == "1"
    
    if force_schema_gen or not schema_path.exists():
        if force_schema_gen and schema_path.exists():
            print(f"âš ï¸  DRG_FORCE_SCHEMA_GEN=1: Mevcut schema dosyasÄ± yok sayÄ±lÄ±yor, metinden yeniden oluÅŸturuluyor...")
        else:
            print(f"âš ï¸  Schema dosyasÄ± bulunamadÄ±: {schema_path}")
            print(f"   Metinden otomatik schema oluÅŸturuluyor...")
            print(f"   ğŸ’¡ Her zaman otomatik schema generation iÃ§in: export DRG_FORCE_SCHEMA_GEN=1")
        
        # LLM'i yapÄ±landÄ±r (schema generation Ã¶ncesi)
        from drg.extract import _configure_llm_auto, generate_schema_from_text
        _configure_llm_auto()
        print("   ğŸ”§ LLM konfigÃ¼rasyonu yapÄ±ldÄ±")
        print("   ğŸ¤– LLM ile metin analiz ediliyor ve uygun ÅŸema oluÅŸturuluyor...")
        print("   â³ Bu iÅŸlem birkaÃ§ saniye sÃ¼rebilir...")
        
        # Metinden otomatik ÅŸema oluÅŸtur (EnhancedDRGSchema dÃ¶ndÃ¼rÃ¼r)
        schema = generate_schema_from_text(text)
        
        # OluÅŸturulan ÅŸemayÄ± kaydet (inputs klasÃ¶rÃ¼ne de kaydet ki sonraki Ã§alÄ±ÅŸtÄ±rmalarda kullanÄ±labilsin)
        save_schema(schema, str(schema_path))
        print(f"   âœ… Otomatik enhanced schema oluÅŸturuldu ve kaydedildi: {schema_path}")
        if isinstance(schema, EnhancedDRGSchema):
            total_relations = sum(len(rg.relations) for rg in schema.relation_groups)
            print(f"      {len(schema.entity_types)} entity type, {len(schema.relation_groups)} relation group, {total_relations} relation")
            if total_relations < 25:
                print(f"      âš ï¸  UyarÄ±: Schema'da sadece {total_relations} relation var. Daha zengin bir KG iÃ§in 30-50+ relation Ã¶nerilir.")
        else:
            print(f"      {len(schema.entities)} entity, {len(schema.relations)} relation")
    else:
        print(f"ğŸ“„ Schema yÃ¼kleniyor: {schema_path}")
        print(f"   ğŸ’¡ Metinden yeniden schema oluÅŸturmak iÃ§in: export DRG_FORCE_SCHEMA_GEN=1")
        schema = load_schema(schema_path)
        if isinstance(schema, EnhancedDRGSchema):
            total_relations = sum(len(rg.relations) for rg in schema.relation_groups)
            print(f"   âœ… Enhanced schema yÃ¼klendi: {len(schema.entity_types)} entity type, {len(schema.relation_groups)} relation group, {total_relations} relation")
        else:
            print(f"   âœ… Legacy schema yÃ¼klendi: {len(schema.entities)} entity, {len(schema.relations)} relation")
        # LLM'i yapÄ±landÄ±r (extraction Ã¶ncesi)
        print("   ğŸ”§ LLM konfigÃ¼rasyonu yapÄ±lÄ±yor...")
        from drg.extract import _configure_llm_auto
        _configure_llm_auto()
        print("   âœ… LLM konfigÃ¼rasyonu tamamlandÄ±")
    
    try:
        print("   ğŸ”„ Chunk-based KG extraction baÅŸlatÄ±lÄ±yor...")
        print(f"   ğŸ“Š {len(chunks)} chunk Ã¼zerinde extraction yapÄ±lacak")
        extractor = KGExtractor(schema)
        
        # Her chunk iÃ§in extraction yap ve birleÅŸtir
        all_entities = set()  # Duplicate'leri Ã¶nlemek iÃ§in set kullan
        all_triples = set()
        
        for i, chunk in enumerate(chunks, 1):
            print(f"   ğŸ” Chunk {i}/{len(chunks)} iÅŸleniyor ({chunk.token_count} tokens)...")
            try:
                result = extractor.forward(chunk.text)
                
                # Parse entities and relations
                entities_str = result.entities if hasattr(result, 'entities') else "[]"
                relations_str = result.relations if hasattr(result, 'relations') else "[]"
                
                entities = json.loads(entities_str) if isinstance(entities_str, str) else entities_str
                relations = json.loads(relations_str) if isinstance(relations_str, str) else relations_str
                
                # Convert to tuples ve ekle
                for e in entities:
                    if isinstance(e, (list, tuple)) and len(e) >= 2:
                        all_entities.add((str(e[0]), str(e[1])))
                
                for r in relations:
                    if isinstance(r, (list, tuple)) and len(r) >= 3:
                        all_triples.add((str(r[0]), str(r[1]), str(r[2])))
                
                print(f"      âœ… {len([e for e in entities if isinstance(e, (list, tuple)) and len(e) >= 2])} entity, "
                      f"{len([r for r in relations if isinstance(r, (list, tuple)) and len(r) >= 3])} relation bulundu")
            except Exception as e:
                print(f"      âš ï¸  Chunk {i} extraction hatasÄ±: {e}")
                continue
        
        # Set'leri list'e Ã§evir
        entities_list = list(all_entities)
        triples_list = list(all_triples)
        
        # Schema validation uygula (schema'da olmayan relation'larÄ± filtrele)
        print(f"   ğŸ” Schema validation uygulanÄ±yor...")
        
        # Entity type'larÄ± schema'dan al
        if isinstance(schema, EnhancedDRGSchema):
            entity_names = {et.name for et in schema.entity_types}
        else:
            entity_names = {e.name for e in schema.entities}
        
        # Valid entity'leri filtrele
        valid_entities = [(name, etype) for name, etype in entities_list if etype in entity_names]
        
        # Valid relation'larÄ± filtrele
        if isinstance(schema, EnhancedDRGSchema):
            valid_triples = []
            for s, r, o in triples_list:
                s_type = next((etype for name, etype in valid_entities if name == s), None)
                o_type = next((etype for name, etype in valid_entities if name == o), None)
                if s_type and o_type and schema.is_valid_relation(r, s_type, o_type):
                    valid_triples.append((s, r, o))
        else:
            rel_types = {(r.src, r.name, r.dst) for r in schema.relations}
            valid_triples = []
            for s, r, o in triples_list:
                s_type = next((etype for name, etype in valid_entities if name == s), None)
                o_type = next((etype for name, etype in valid_entities if name == o), None)
                if s_type and o_type and (s_type, r, o_type) in rel_types:
                    valid_triples.append((s, r, o))
        
        # Filtered sonuÃ§larÄ± kullan
        entities_list = valid_entities
        triples_list = valid_triples
        
        print(f"âœ… Toplam {len(entities_list)} unique entity ve {len(triples_list)} unique relation extract edildi (schema validation sonrasÄ±)")
        if len(all_entities) > len(entities_list) or len(all_triples) > len(triples_list):
            print(f"   âš ï¸  Schema validation: {len(all_entities) - len(entities_list)} entity, {len(all_triples) - len(triples_list)} relation filtrelendi")
        print(f"   Ã–rnek entities: {entities_list[:5]}")
        print(f"   Ã–rnek relations: {triples_list[:3]}")
        
    except Exception as e:
        print(f"âš ï¸  KG extraction hatasÄ±: {e}")
        print("   Mock data kullanÄ±lÄ±yor...")
        entities_list = [
            ("Apple", "Company"),
            ("Steve Jobs", "Person"),
            ("Tim Cook", "Person"),
            ("iPhone", "Product"),
            ("iPad", "Product"),
            ("Mac", "Product"),
            ("Cupertino", "Location"),
            ("2007", "Year"),
        ]
        triples_list = [
            ("Apple", "founded_by", "Steve Jobs"),
            ("Apple", "ceo_of", "Tim Cook"),
            ("Apple", "produces", "iPhone"),
            ("Apple", "produces", "iPad"),
            ("Apple", "produces", "Mac"),
            ("Apple", "located_in", "Cupertino"),
            ("iPhone", "released_in", "2007"),
        ]
        print(f"   Mock: {len(entities_list)} entity ve {len(triples_list)} relation")
    
    # Step 3: EnhancedKG OluÅŸtur
    print("\n" + "=" * 70)
    print("3. ENHANCED KG OLUÅTURMA")
    print("=" * 70)
    
    enhanced_kg = EnhancedKG()
    
    # Nodes ekle
    for entity_id, entity_type in entities_list:
        node = KGNode(
            id=entity_id,
            type=entity_type,
            properties={},
            metadata={},
        )
        enhanced_kg.add_node(node)
    
    # Relation description ve detail'lerini schema'dan al
    relation_descriptions = {}
    relation_details = {}
    
    if isinstance(schema, EnhancedDRGSchema):
        # Enhanced schema'dan relation description ve detail'lerini al
        for rg in schema.relation_groups:
            for r in rg.relations:
                if r.description:
                    relation_descriptions[r.name] = r.description
                if r.detail:
                    relation_details[r.name] = r.detail
    else:
        # Legacy schema'dan
        for r in schema.relations:
            if hasattr(r, 'description') and r.description:
                relation_descriptions[r.name] = r.description
            if hasattr(r, 'detail') and r.detail:
                relation_details[r.name] = r.detail
    
    # Edges ekle
    for source, relation, target in triples_list:
        # Self-loop kontrolÃ¼: source ve target aynÄ±ysa atla
        if source == target:
            continue
        
        # Source ve target node'larÄ± yoksa ekle
        if source not in enhanced_kg.nodes:
            enhanced_kg.add_node(KGNode(id=source, type=None))
        if target not in enhanced_kg.nodes:
            enhanced_kg.add_node(KGNode(id=target, type=None))
        
        # Relationship detail: Schema'daki detail varsa kullan, yoksa description, yoksa basit format
        if relation in relation_details:
            relationship_detail = relation_details[relation]
        elif relation in relation_descriptions:
            relationship_detail = relation_descriptions[relation]
        else:
            relationship_detail = f"{source} {relation} {target}"
        
        try:
            edge = KGEdge(
                source=source,
                target=target,
                relationship_type=relation,
                relationship_detail=relationship_detail,
                metadata={},
            )
            enhanced_kg.add_edge(edge)
        except ValueError as e:
            # Self-loop veya diÄŸer validation hatalarÄ±nÄ± atla
            print(f"   âš ï¸  Edge atlandÄ± ({source} --[{relation}]--> {target}): {e}")
            continue
    
    print(f"âœ… EnhancedKG oluÅŸturuldu: {len(enhanced_kg.nodes)} node, {len(enhanced_kg.edges)} edge")
    
    # Step 4: Entity Embedding'leri Ekle
    print("\n" + "=" * 70)
    print("4. ENTITY EMBEDDING'LERÄ° EKLEME")
    print("=" * 70)
    
    # Embedding provider oluÅŸtur
    try:
        # OpenRouter Ã¼zerinden embedding (eÄŸer destekleniyorsa)
        embedding_provider = create_embedding_provider(
            provider="openrouter",
            model="openai/text-embedding-3-small",  # OpenAI embedding via OpenRouter
        )
        print("   OpenRouter embedding provider kullanÄ±lÄ±yor")
    except Exception as e:
        print(f"   OpenRouter embedding yok ({e}), local embedding deneniyor...")
        try:
            embedding_provider = create_embedding_provider(
                provider="local",
                model="sentence-transformers/all-MiniLM-L6-v2",
            )
            print("   Local embedding provider kullanÄ±lÄ±yor")
        except Exception as e2:
            print(f"   Local embedding yok ({e2}), mock embedding kullanÄ±lÄ±yor...")
            class MockEmbeddingProvider:
                def __init__(self):
                    import hashlib
                    self.hash_func = hashlib.md5
                
                def embed(self, text: str):
                    """Deterministic embedding based on text hash."""
                    import random
                    seed = int(self.hash_func(text.encode()).hexdigest()[:8], 16)
                    random.seed(seed)
                    return [random.random() for _ in range(384)]
                
                def embed_batch(self, texts):
                    return [self.embed(text) for text in texts]
                
                def get_dimension(self):
                    return 384
                
                def get_model_name(self):
                    return "mock/deterministic"
            
            embedding_provider = MockEmbeddingProvider()
            print("   Mock embedding provider kullanÄ±lÄ±yor (deterministic)")
    
    # Entity embedding'lerini ekle
    entity_texts = {node_id: node_id for node_id in enhanced_kg.nodes.keys()}
    enhanced_kg.add_entity_embeddings(embedding_provider, entity_texts)
    
    embedded_count = sum(1 for node in enhanced_kg.nodes.values() if node.embedding is not None)
    print(f"âœ… {embedded_count}/{len(enhanced_kg.nodes)} node'a embedding eklendi")
    
    # Step 5: Clustering ve Community Reports
    print("\n" + "=" * 70)
    print("5. CLUSTERING VE COMMUNITY REPORTS")
    print("=" * 70)
    
    try:
        # Clustering yap
        clustering_algorithm = create_clustering_algorithm(
            algorithm="louvain",
        )
        
        # KG'yi NetworkX formatÄ±na Ã§evir (clustering iÃ§in)
        import networkx as nx
        G = nx.Graph()
        for node_id in enhanced_kg.nodes.keys():
            G.add_node(node_id)
        for edge in enhanced_kg.edges:
            G.add_edge(edge.source, edge.target)
        
        # Clustering yap
        clustering_result = clustering_algorithm.cluster(G)
        
        # EnhancedKG'ye cluster'larÄ± ekle
        # clustering_result is List[Cluster] from drg.clustering.algorithms
        from drg.graph.kg_core import Cluster as KGCluster
        for clustering_cluster in clustering_result:
            # Convert clustering Cluster to KG Cluster
            kg_cluster = KGCluster(
                id=f"cluster_{clustering_cluster.cluster_id}",
                node_ids=set(clustering_cluster.nodes),
                metadata=clustering_cluster.metadata,
            )
            enhanced_kg.add_cluster(kg_cluster)
        
        print(f"âœ… {len(clustering_result)} cluster oluÅŸturuldu")
        
        # Community reports oluÅŸtur
        report_generator = CommunityReportGenerator(enhanced_kg)
        reports = report_generator.generate_all_reports()
        
        print(f"âœ… {len(reports)} community report oluÅŸturuldu")
        for report in reports[:2]:
            print(f"   - {report.cluster_id}: {len(report.top_actors)} actors, {len(report.top_relationships)} relations")
        
    except Exception as e:
        print(f"âš ï¸  Clustering hatasÄ±: {e}")
        print("   Community reports olmadan devam ediliyor...")
        reports = []
    
    # Step 6: GraphRAG Retrieval
    print("\n" + "=" * 70)
    print("6. GRAPHRAG RETRIEVAL")
    print("=" * 70)
    
    try:
        retriever = create_graphrag_retriever(
            embedding_provider=embedding_provider,
            knowledge_graph=enhanced_kg,
            community_reports=reports if reports else None,
            similarity_threshold=0.3,  # DÃ¼ÅŸÃ¼k threshold (mock embeddings iÃ§in)
            max_hops=2,
        )
        
        # Test query'leri
        queries = [
            "What products does Apple produce?",
            "Who is the CEO of Apple?",
            "Where is Apple located?",
        ]
        
        for query in queries:
            print(f"\n   Query: {query}")
            context = retriever.retrieve(query, k_entities=5, k_reports=3)
            
            print(f"   âœ… Seed entities: {context.seed_entities}")
            print(f"   âœ… Subgraph: {len(context.kg_subgraph['nodes'])} nodes, {len(context.kg_subgraph['edges'])} edges")
            print(f"   âœ… Entities: {len(context.entities)}")
            print(f"   âœ… Relationships: {len(context.relationships)}")
            print(f"   âœ… Community reports: {len(context.community_reports)}")
            
            if context.entities:
                print(f"      Entities: {', '.join(context.entities[:5])}")
            if context.relationships:
                print(f"      Sample relation: {context.relationships[0]['source']} --[{context.relationships[0]['relationship_type']}]--> {context.relationships[0]['target']}")
    
    except Exception as e:
        print(f"âš ï¸  GraphRAG retrieval hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
    
    # Step 7: Output'larÄ± Kaydet
    print("\n" + "=" * 70)
    print("7. OUTPUT'LARI KAYDETME")
    print("=" * 70)
    
    # Output dosya yollarÄ±
    kg_output_path = outputs_dir / f"{example_name}_kg.json"
    summary_output_path = outputs_dir / f"{example_name}_summary.json"
    reports_output_path = outputs_dir / f"{example_name}_community_reports.json"
    schema_output_path = outputs_dir / f"{example_name}_schema.json"
    
    # Schema'yÄ± output olarak kaydet (kullanÄ±lan schema - Enhanced veya Legacy)
    save_schema(schema, str(schema_output_path))
    print(f"âœ… Schema kaydedildi: {schema_output_path}")
    
    # KG'yi kaydet
    kg_json = enhanced_kg.to_json(indent=2)
    with open(kg_output_path, "w", encoding="utf-8") as f:
        f.write(kg_json)
    print(f"âœ… KG kaydedildi: {kg_output_path}")
    
    # Community reports'u kaydet
    if reports:
        report_generator.export_reports_json(reports, str(reports_output_path))
        print(f"âœ… Community reports kaydedildi: {reports_output_path}")
    else:
        print(f"â„¹ï¸  Community reports yok (clustering gerekli)")
    
    # Pipeline summary
    summary = {
        "example_name": example_name,
        "input_files": {
            "schema": str(schema_path),
            "text": str(text_path),
        },
        "chunks": len(chunks),
        "entities": len(entities_list),
        "relations": len(triples_list),
        "kg_nodes": len(enhanced_kg.nodes),
        "kg_edges": len(enhanced_kg.edges),
        "embedded_nodes": embedded_count,
        "clusters": len(enhanced_kg.clusters),
        "community_reports": len(reports),
        "output_files": {
            "schema": str(schema_output_path),
            "kg": str(kg_output_path),
            "summary": str(summary_output_path),
            "community_reports": str(reports_output_path) if reports else None,
        },
    }
    
    with open(summary_output_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"âœ… Pipeline summary kaydedildi: {summary_output_path}")
    
    print("\n" + "=" * 70)
    print("âœ… GRAPHRAG PIPELINE TAMAMLANDI!")
    print("=" * 70)
    print(f"\nğŸ“Š Ã–zet:")
    print(f"   - Chunks: {summary['chunks']}")
    print(f"   - KG Nodes: {summary['kg_nodes']}")
    print(f"   - KG Edges: {summary['kg_edges']}")
    print(f"   - Embedded Nodes: {summary['embedded_nodes']}")
    print(f"   - Clusters: {summary['clusters']}")
    print(f"   - Community Reports: {summary['community_reports']}")


if __name__ == "__main__":
    main()

