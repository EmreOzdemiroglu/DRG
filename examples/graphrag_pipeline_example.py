#!/usr/bin/env python3
"""
GraphRAG Pipeline Example - Tam Pipeline Testi

GraphRAG yapƒ±sƒ±na uygun olarak:
1. Chunking
2. KG Extraction
3. Entity Embedding'leri Ekleme
4. Community Report Olu≈üturma
5. GraphRAG Retrieval (KG traversal + Community reports)

Kullanƒ±m:
    python examples/graphrag_pipeline_example.py [example_name]

√ñrnek:
    python examples/graphrag_pipeline_example.py 1
    python examples/graphrag_pipeline_example.py 1example
    python examples/graphrag_pipeline_example.py example1  # Otomatik 1example'a √ßevrilir
"""

import os
import sys
import json
import ssl
from pathlib import Path

# SSL sorununu √ß√∂z (sandbox ortamƒ±nda /Library/Frameworks/ eri≈üim kƒ±sƒ±tlamasƒ± i√ßin)
# Environment variables
os.environ['PYTHONHTTPSVERIFY'] = '0'
os.environ['CURL_CA_BUNDLE'] = ''
os.environ['REQUESTS_CA_BUNDLE'] = ''

# SSL context'i unverified olarak ayarla
ssl._create_default_https_context = ssl._create_unverified_context

# requests adapters SSL context y√ºkleme i≈ülemini bypass et
try:
    import requests
    from requests import adapters
    if hasattr(adapters, '_preloaded_ssl_context'):
        try:
            unverified_context = ssl.create_default_context()
            unverified_context.check_hostname = False
            unverified_context.verify_mode = ssl.CERT_NONE
            adapters._preloaded_ssl_context = unverified_context
        except:
            adapters._preloaded_ssl_context = None
except:
    pass

# urllib3 SSL uyarƒ±larƒ±nƒ± kapat
try:
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
except:
    pass

# Proje root'u path'e ekle
sys.path.insert(0, str(Path(__file__).parent.parent))

from typing import Union

from drg.chunking import create_chunker
from drg.embedding import create_embedding_provider
from drg.extract import extract_typed, KGExtractor
from drg.schema import DRGSchema, EnhancedDRGSchema, Entity, Relation, EntityType, RelationGroup
from drg.graph import KG
from drg.graph.kg_core import EnhancedKG, KGNode, KGEdge
from drg.graph.community_report import CommunityReportGenerator
from drg.clustering import create_clustering_algorithm
from drg.retrieval import create_graphrag_retriever


def load_schema(schema_path: str) -> Union[DRGSchema, EnhancedDRGSchema]:
    """Schema'yƒ± JSON dosyasƒ±ndan y√ºkle (Enhanced veya legacy format destekler)."""
    with open(schema_path, "r", encoding="utf-8") as f:
        schema_data = json.load(f)
    
    # Enhanced schema formatƒ±nƒ± kontrol et
    if "entity_types" in schema_data:
        from drg.schema import EntityType, RelationGroup
        entity_types = [
            EntityType(
                name=et["name"],
                description=et.get("description", ""),
                examples=et.get("examples", []),
                properties=et.get("properties", {})
            )
            for et in schema_data["entity_types"]
        ]
        
        relation_groups = []
        for rg_data in schema_data.get("relation_groups", []):
            relations = [
                Relation(
                    name=r["name"],
                    src=r["source"],
                    dst=r["target"],
                    description=r.get("description", ""),
                    detail=r.get("detail", "")
                )
                for r in rg_data.get("relations", [])
            ]
            relation_groups.append(RelationGroup(
                name=rg_data["name"],
                description=rg_data.get("description", ""),
                relations=relations,
                examples=rg_data.get("examples", [])
            ))
        
        return EnhancedDRGSchema(
            entity_types=entity_types,
            relation_groups=relation_groups,
            auto_discovery=schema_data.get("auto_discovery", False)
        )
    else:
        # Legacy format
        entities = [Entity(e["name"]) for e in schema_data.get("entities", [])]
        relations = [
            Relation(
                r["name"], 
                r.get("source", r.get("src", "")), 
                r.get("target", r.get("dst", "")),
                description=r.get("description", ""),
                detail=r.get("detail", "")
            )
            for r in schema_data.get("relations", [])
        ]
        
        return DRGSchema(entities=entities, relations=relations)


def save_schema(schema: Union[DRGSchema, EnhancedDRGSchema], schema_path: str) -> None:
    """Schema'yƒ± JSON dosyasƒ±na kaydet."""
    from drg.schema import EnhancedDRGSchema as EDRGS
    from pathlib import Path
    
    schema_path_obj = Path(schema_path)
    schema_path_obj.parent.mkdir(parents=True, exist_ok=True)
    
    if isinstance(schema, EDRGS):
        # Enhanced schema formatƒ±
        schema_dict = {
            "entity_types": [
                {
                    "name": et.name,
                    "description": et.description,
                    "examples": et.examples,
                    "properties": et.properties
                }
                for et in schema.entity_types
            ],
            "relation_groups": [
                {
                    "name": rg.name,
                    "description": rg.description,
                    "relations": [
                        {
                            "name": r.name,
                            "source": r.src,
                            "target": r.dst,
                            "description": r.description,
                            "detail": r.detail
                        }
                        for r in rg.relations
                    ],
                    "examples": rg.examples
                }
                for rg in schema.relation_groups
            ],
            "auto_discovery": schema.auto_discovery
        }
    else:
        # Legacy format
        schema_dict = {
            "entities": [{"name": e.name} for e in schema.entities],
            "relations": [
                {
                    "name": r.name,
                    "source": r.src,
                    "target": r.dst,
                    "description": r.description,
                    "detail": r.detail
                }
                for r in schema.relations
            ]
        }
    
    with open(schema_path, "w", encoding="utf-8") as f:
        json.dump(schema_dict, f, indent=2, ensure_ascii=False)


def main():
    """GraphRAG pipeline'ƒ±nƒ± test et."""
    
    # Example name belirle - sayƒ± ba≈üta formatƒ± kullan (1example, 2example, etc.)
    raw_name = sys.argv[1] if len(sys.argv) > 1 else "1"
    
    # Eƒüer sadece sayƒ± verilmi≈üse "Nexample" formatƒ±na √ßevir
    if raw_name.isdigit():
        example_name = f"{raw_name}example"
    elif raw_name.startswith("example"):
        # Eski format: "example1" -> "1example"
        num = raw_name.replace("example", "")
        example_name = f"{num}example" if num.isdigit() else raw_name
    else:
        example_name = raw_name
    
    print("=" * 70)
    print(f"GraphRAG Pipeline Test - {example_name.upper()}")
    print("=" * 70)
    
    # Klas√∂r yapƒ±sƒ±nƒ± olu≈ütur
    inputs_dir = Path("inputs")
    outputs_dir = Path("outputs")
    inputs_dir.mkdir(exist_ok=True)
    outputs_dir.mkdir(exist_ok=True)
    
    # Schema ve text dosya yollarƒ± - sayƒ± ba≈üta format
    schema_path = inputs_dir / f"{example_name}_schema.json"
    text_path = inputs_dir / f"{example_name}_text.txt"
    
    # API Key ayarla (sadece environment variable'dan oku)
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    if openrouter_key:
        os.environ["OPENROUTER_API_KEY"] = openrouter_key
        print("‚úÖ OPENROUTER_API_KEY environment variable'dan okundu")
    else:
        print("‚ö†Ô∏è  OPENROUTER_API_KEY bulunamadƒ±. OpenRouter servisleri √ßalƒ±≈ümayabilir.")
        print("   API key'i ayarlamak i√ßin: export OPENROUTER_API_KEY='your-key'")
    if not os.getenv("DRG_MODEL"):
        os.environ["DRG_MODEL"] = "openrouter/anthropic/claude-3-haiku"
    
    # Metni dosyadan y√ºkle
    if not text_path.exists():
        print(f"‚ö†Ô∏è  Metin dosyasƒ± bulunamadƒ±: {text_path}")
        print(f"   L√ºtfen {text_path} dosyasƒ±nƒ± olu≈üturun")
        return
    
    print(f"üìÑ Metin dosyasƒ± y√ºkleniyor: {text_path}")
    with open(text_path, "r", encoding="utf-8") as f:
        text = f.read()
    
    print(f"\nüìÑ Metin uzunluƒüu: {len(text)} karakter")
    print(f"   Kelime sayƒ±sƒ±: {len(text.split())}")
    
    # Step 1: Chunking
    print("\n" + "=" * 70)
    print("1. CHUNKING")
    print("=" * 70)
    
    # Declarative chunking: preset kullanarak
    chunker = create_chunker(preset="graphrag")
    
    chunks = chunker.chunk(
        text=text,
        origin_dataset="apple_corpus",
        origin_file="apple_history.txt",
    )
    
    print(f"‚úÖ {len(chunks)} chunk olu≈üturuldu")
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"   Chunk {i}: {chunk.token_count} tokens, ID: {chunk.chunk_id}")
    
    # Step 2: KG Extraction
    print("\n" + "=" * 70)
    print("2. KNOWLEDGE GRAPH EXTRACTION")
    print("=" * 70)
    
    # Schema'yƒ± dosyadan y√ºkle veya metinden otomatik olu≈ütur
    if not schema_path.exists():
        print(f"‚ö†Ô∏è  Schema dosyasƒ± bulunamadƒ±: {schema_path}")
        print(f"   Metinden otomatik schema olu≈üturuluyor...")
        # LLM'i yapƒ±landƒ±r (schema generation √∂ncesi)
        from drg.extract import _configure_llm_auto, generate_schema_from_text
        _configure_llm_auto()
        print("   üîß LLM konfig√ºrasyonu yapƒ±ldƒ±")
        print("   ü§ñ LLM ile metin analiz ediliyor ve uygun ≈üema olu≈üturuluyor...")
        
        # Metinden otomatik ≈üema olu≈ütur (EnhancedDRGSchema d√∂nd√ºr√ºr)
        schema = generate_schema_from_text(text)
        
        # Olu≈üturulan ≈üemayƒ± kaydet
        save_schema(schema, str(schema_path))
        print(f"   ‚úÖ Otomatik enhanced schema olu≈üturuldu ve kaydedildi: {schema_path}")
        if isinstance(schema, EnhancedDRGSchema):
            total_relations = sum(len(rg.relations) for rg in schema.relation_groups)
            print(f"      {len(schema.entity_types)} entity type, {len(schema.relation_groups)} relation group, {total_relations} relation")
        else:
            print(f"      {len(schema.entities)} entity, {len(schema.relations)} relation")
    else:
        print(f"üìÑ Schema y√ºkleniyor: {schema_path}")
        schema = load_schema(schema_path)
        if isinstance(schema, EnhancedDRGSchema):
            total_relations = sum(len(rg.relations) for rg in schema.relation_groups)
            print(f"   ‚úÖ Enhanced schema y√ºklendi: {len(schema.entity_types)} entity type, {len(schema.relation_groups)} relation group, {total_relations} relation")
        else:
            print(f"   ‚úÖ Legacy schema y√ºklendi: {len(schema.entities)} entity, {len(schema.relations)} relation")
        # LLM'i yapƒ±landƒ±r (extraction √∂ncesi)
        print("   üîß LLM konfig√ºrasyonu yapƒ±lƒ±yor...")
        from drg.extract import _configure_llm_auto
        _configure_llm_auto()
        print("   ‚úÖ LLM konfig√ºrasyonu tamamlandƒ±")
    
    try:
        print("   üîÑ Chunk-based KG extraction ba≈ülatƒ±lƒ±yor...")
        print(f"   üìä {len(chunks)} chunk √ºzerinde extraction yapƒ±lacak")
        extractor = KGExtractor(schema)
        
        # Her chunk i√ßin extraction yap ve birle≈ütir
        all_entities = set()  # Duplicate'leri √∂nlemek i√ßin set kullan
        all_triples = set()
        
        for i, chunk in enumerate(chunks, 1):
            print(f"   üîç Chunk {i}/{len(chunks)} i≈üleniyor ({chunk.token_count} tokens)...")
            try:
                result = extractor.forward(chunk.text)
                
                # Parse entities and relations
                entities_str = result.entities if hasattr(result, 'entities') else "[]"
                relations_str = result.relations if hasattr(result, 'relations') else "[]"
                
                entities = json.loads(entities_str) if isinstance(entities_str, str) else entities_str
                relations = json.loads(relations_str) if isinstance(relations_str, str) else relations_str
                
                # Convert to tuples ve ekle
                for e in entities:
                    if isinstance(e, (list, tuple)) and len(e) >= 2:
                        all_entities.add((str(e[0]), str(e[1])))
                
                for r in relations:
                    if isinstance(r, (list, tuple)) and len(r) >= 3:
                        all_triples.add((str(r[0]), str(r[1]), str(r[2])))
                
                print(f"      ‚úÖ {len([e for e in entities if isinstance(e, (list, tuple)) and len(e) >= 2])} entity, "
                      f"{len([r for r in relations if isinstance(r, (list, tuple)) and len(r) >= 3])} relation bulundu")
            except Exception as e:
                print(f"      ‚ö†Ô∏è  Chunk {i} extraction hatasƒ±: {e}")
                continue
        
        # Set'leri list'e √ßevir
        entities_list = list(all_entities)
        triples_list = list(all_triples)
        
        print(f"‚úÖ Toplam {len(entities_list)} unique entity ve {len(triples_list)} unique relation extract edildi")
        print(f"   √ñrnek entities: {entities_list[:5]}")
        print(f"   √ñrnek relations: {triples_list[:3]}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  KG extraction hatasƒ±: {e}")
        print("   Mock data kullanƒ±lƒ±yor...")
        entities_list = [
            ("Apple", "Company"),
            ("Steve Jobs", "Person"),
            ("Tim Cook", "Person"),
            ("iPhone", "Product"),
            ("iPad", "Product"),
            ("Mac", "Product"),
            ("Cupertino", "Location"),
            ("2007", "Year"),
        ]
        triples_list = [
            ("Apple", "founded_by", "Steve Jobs"),
            ("Apple", "ceo_of", "Tim Cook"),
            ("Apple", "produces", "iPhone"),
            ("Apple", "produces", "iPad"),
            ("Apple", "produces", "Mac"),
            ("Apple", "located_in", "Cupertino"),
            ("iPhone", "released_in", "2007"),
        ]
        print(f"   Mock: {len(entities_list)} entity ve {len(triples_list)} relation")
    
    # Step 3: EnhancedKG Olu≈ütur
    print("\n" + "=" * 70)
    print("3. ENHANCED KG OLU≈ûTURMA")
    print("=" * 70)
    
    enhanced_kg = EnhancedKG()
    
    # Nodes ekle
    for entity_id, entity_type in entities_list:
        node = KGNode(
            id=entity_id,
            type=entity_type,
            properties={},
            metadata={},
        )
        enhanced_kg.add_node(node)
    
    # Relation description ve detail'lerini schema'dan al
    relation_descriptions = {}
    relation_details = {}
    
    if isinstance(schema, EnhancedDRGSchema):
        # Enhanced schema'dan relation description ve detail'lerini al
        for rg in schema.relation_groups:
            for r in rg.relations:
                if r.description:
                    relation_descriptions[r.name] = r.description
                if r.detail:
                    relation_details[r.name] = r.detail
    else:
        # Legacy schema'dan
        for r in schema.relations:
            if hasattr(r, 'description') and r.description:
                relation_descriptions[r.name] = r.description
            if hasattr(r, 'detail') and r.detail:
                relation_details[r.name] = r.detail
    
    # Edges ekle
    for source, relation, target in triples_list:
        # Self-loop kontrol√º: source ve target aynƒ±ysa atla
        if source == target:
            continue
        
        # Source ve target node'larƒ± yoksa ekle
        if source not in enhanced_kg.nodes:
            enhanced_kg.add_node(KGNode(id=source, type=None))
        if target not in enhanced_kg.nodes:
            enhanced_kg.add_node(KGNode(id=target, type=None))
        
        # Relationship detail: Schema'daki detail varsa kullan, yoksa description, yoksa basit format
        if relation in relation_details:
            relationship_detail = relation_details[relation]
        elif relation in relation_descriptions:
            relationship_detail = relation_descriptions[relation]
        else:
            relationship_detail = f"{source} {relation} {target}"
        
        try:
            edge = KGEdge(
                source=source,
                target=target,
                relationship_type=relation,
                relationship_detail=relationship_detail,
                metadata={},
            )
            enhanced_kg.add_edge(edge)
        except ValueError as e:
            # Self-loop veya diƒüer validation hatalarƒ±nƒ± atla
            print(f"   ‚ö†Ô∏è  Edge atlandƒ± ({source} --[{relation}]--> {target}): {e}")
            continue
    
    print(f"‚úÖ EnhancedKG olu≈üturuldu: {len(enhanced_kg.nodes)} node, {len(enhanced_kg.edges)} edge")
    
    # Step 4: Entity Embedding'leri Ekle
    print("\n" + "=" * 70)
    print("4. ENTITY EMBEDDING'LERƒ∞ EKLEME")
    print("=" * 70)
    
    # Embedding provider olu≈ütur
    try:
        # OpenRouter √ºzerinden embedding (eƒüer destekleniyorsa)
        embedding_provider = create_embedding_provider(
            provider="openrouter",
            model="text-embedding-3-small",  # OpenAI embedding via OpenRouter
        )
        print("   OpenRouter embedding provider kullanƒ±lƒ±yor")
    except Exception as e:
        print(f"   OpenRouter embedding yok ({e}), local embedding deneniyor...")
        try:
            embedding_provider = create_embedding_provider(
                provider="local",
                model="sentence-transformers/all-MiniLM-L6-v2",
            )
            print("   Local embedding provider kullanƒ±lƒ±yor")
        except Exception as e2:
            print(f"   Local embedding yok ({e2}), mock embedding kullanƒ±lƒ±yor...")
            class MockEmbeddingProvider:
                def __init__(self):
                    import hashlib
                    self.hash_func = hashlib.md5
                
                def embed(self, text: str):
                    """Deterministic embedding based on text hash."""
                    import random
                    seed = int(self.hash_func(text.encode()).hexdigest()[:8], 16)
                    random.seed(seed)
                    return [random.random() for _ in range(384)]
                
                def embed_batch(self, texts):
                    return [self.embed(text) for text in texts]
                
                def get_dimension(self):
                    return 384
                
                def get_model_name(self):
                    return "mock/deterministic"
            
            embedding_provider = MockEmbeddingProvider()
            print("   Mock embedding provider kullanƒ±lƒ±yor (deterministic)")
    
    # Entity embedding'lerini ekle
    entity_texts = {node_id: node_id for node_id in enhanced_kg.nodes.keys()}
    enhanced_kg.add_entity_embeddings(embedding_provider, entity_texts)
    
    embedded_count = sum(1 for node in enhanced_kg.nodes.values() if node.embedding is not None)
    print(f"‚úÖ {embedded_count}/{len(enhanced_kg.nodes)} node'a embedding eklendi")
    
    # Step 5: Clustering ve Community Reports
    print("\n" + "=" * 70)
    print("5. CLUSTERING VE COMMUNITY REPORTS")
    print("=" * 70)
    
    try:
        # Clustering yap
        clustering_algorithm = create_clustering_algorithm(
            algorithm="louvain",
        )
        
        # KG'yi NetworkX formatƒ±na √ßevir (clustering i√ßin)
        import networkx as nx
        G = nx.Graph()
        for node_id in enhanced_kg.nodes.keys():
            G.add_node(node_id)
        for edge in enhanced_kg.edges:
            G.add_edge(edge.source, edge.target)
        
        # Clustering yap
        clustering_result = clustering_algorithm.cluster(G)
        
        # EnhancedKG'ye cluster'larƒ± ekle
        # clustering_result is List[Cluster] from drg.clustering.algorithms
        from drg.graph.kg_core import Cluster as KGCluster
        for clustering_cluster in clustering_result:
            # Convert clustering Cluster to KG Cluster
            kg_cluster = KGCluster(
                id=f"cluster_{clustering_cluster.cluster_id}",
                node_ids=set(clustering_cluster.nodes),
                metadata=clustering_cluster.metadata,
            )
            enhanced_kg.add_cluster(kg_cluster)
        
        print(f"‚úÖ {len(clustering_result)} cluster olu≈üturuldu")
        
        # Community reports olu≈ütur
        report_generator = CommunityReportGenerator(enhanced_kg)
        reports = report_generator.generate_all_reports()
        
        print(f"‚úÖ {len(reports)} community report olu≈üturuldu")
        for report in reports[:2]:
            print(f"   - {report.cluster_id}: {len(report.top_actors)} actors, {len(report.top_relationships)} relations")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Clustering hatasƒ±: {e}")
        print("   Community reports olmadan devam ediliyor...")
        reports = []
    
    # Step 6: GraphRAG Retrieval
    print("\n" + "=" * 70)
    print("6. GRAPHRAG RETRIEVAL")
    print("=" * 70)
    
    try:
        retriever = create_graphrag_retriever(
            embedding_provider=embedding_provider,
            knowledge_graph=enhanced_kg,
            community_reports=reports if reports else None,
            similarity_threshold=0.3,  # D√º≈ü√ºk threshold (mock embeddings i√ßin)
            max_hops=2,
        )
        
        # Test query'leri
        queries = [
            "What products does Apple produce?",
            "Who is the CEO of Apple?",
            "Where is Apple located?",
        ]
        
        for query in queries:
            print(f"\n   Query: {query}")
            context = retriever.retrieve(query, k_entities=5, k_reports=3)
            
            print(f"   ‚úÖ Seed entities: {context.seed_entities}")
            print(f"   ‚úÖ Subgraph: {len(context.kg_subgraph['nodes'])} nodes, {len(context.kg_subgraph['edges'])} edges")
            print(f"   ‚úÖ Entities: {len(context.entities)}")
            print(f"   ‚úÖ Relationships: {len(context.relationships)}")
            print(f"   ‚úÖ Community reports: {len(context.community_reports)}")
            
            if context.entities:
                print(f"      Entities: {', '.join(context.entities[:5])}")
            if context.relationships:
                print(f"      Sample relation: {context.relationships[0]['source']} --[{context.relationships[0]['relationship_type']}]--> {context.relationships[0]['target']}")
    
    except Exception as e:
        print(f"‚ö†Ô∏è  GraphRAG retrieval hatasƒ±: {e}")
        import traceback
        traceback.print_exc()
    
    # Step 7: Output'larƒ± Kaydet
    print("\n" + "=" * 70)
    print("7. OUTPUT'LARI KAYDETME")
    print("=" * 70)
    
    # Output dosya yollarƒ±
    kg_output_path = outputs_dir / f"{example_name}_kg.json"
    summary_output_path = outputs_dir / f"{example_name}_summary.json"
    reports_output_path = outputs_dir / f"{example_name}_community_reports.json"
    schema_output_path = outputs_dir / f"{example_name}_schema.json"
    
    # Schema'yƒ± output olarak kaydet (kullanƒ±lan schema - Enhanced veya Legacy)
    save_schema(schema, str(schema_output_path))
    print(f"‚úÖ Schema kaydedildi: {schema_output_path}")
    
    # KG'yi kaydet
    kg_json = enhanced_kg.to_json(indent=2)
    with open(kg_output_path, "w", encoding="utf-8") as f:
        f.write(kg_json)
    print(f"‚úÖ KG kaydedildi: {kg_output_path}")
    
    # Community reports'u kaydet
    if reports:
        report_generator.export_reports_json(reports, str(reports_output_path))
        print(f"‚úÖ Community reports kaydedildi: {reports_output_path}")
    else:
        print(f"‚ÑπÔ∏è  Community reports yok (clustering gerekli)")
    
    # Pipeline summary
    summary = {
        "example_name": example_name,
        "input_files": {
            "schema": str(schema_path),
            "text": str(text_path),
        },
        "chunks": len(chunks),
        "entities": len(entities_list),
        "relations": len(triples_list),
        "kg_nodes": len(enhanced_kg.nodes),
        "kg_edges": len(enhanced_kg.edges),
        "embedded_nodes": embedded_count,
        "clusters": len(enhanced_kg.clusters),
        "community_reports": len(reports),
        "output_files": {
            "schema": str(schema_output_path),
            "kg": str(kg_output_path),
            "summary": str(summary_output_path),
            "community_reports": str(reports_output_path) if reports else None,
        },
    }
    
    with open(summary_output_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ Pipeline summary kaydedildi: {summary_output_path}")
    
    print("\n" + "=" * 70)
    print("‚úÖ GRAPHRAG PIPELINE TAMAMLANDI!")
    print("=" * 70)
    print(f"\nüìä √ñzet:")
    print(f"   - Chunks: {summary['chunks']}")
    print(f"   - KG Nodes: {summary['kg_nodes']}")
    print(f"   - KG Edges: {summary['kg_edges']}")
    print(f"   - Embedded Nodes: {summary['embedded_nodes']}")
    print(f"   - Clusters: {summary['clusters']}")
    print(f"   - Community Reports: {summary['community_reports']}")


if __name__ == "__main__":
    main()

